# Symptomchecker

## Beispielawendungen

| Name      | URL                         |
|-----------|-----------------------------|
| Mediktor  | [my.mediktor.com](https://my.mediktor.com/de) |
| Ada       | [ada.com](https://ada.com/)             |
| Symptoma  | [symptoma.com](https://www.symptoma.com/) |

: Beispiele Symptomchecker

## Studien

Die Studie "Correlating global trends in COVID-19 cases with online symptom checker self-assessments" von Marc Zobel, Bernhard Knapp, Jama Nateqi und Alistair Martin, veröffentlicht am 10. Februar 2023 in PLOS ONE, untersucht die Beziehung zwischen den Risikobewertungen eines Online-Symptomcheckers und den weltweiten Trends bei COVID-19-Infektionen. Sie analysiert Daten des Symptomcheckers Symptoma (www.symptoma.com) aus 18 Ländern und vergleicht diese mit offiziellen Infektionszahlen, um Korrelationen zu ermitteln. Die Studie zeigt eine durchschnittliche Korrelation von 0,342 zwischen den als risikoreich eingestuften Nutzern und den bestätigten Fällen, wobei diese Korrelation mit der selbstberichteten Gesundheit eines Landes zusammenhängt. Zudem stellt sie fest, dass die Trends im Symptomchecker den offiziellen Zahlen meist um drei Tage vorausgehen. Die Autoren schließen, dass Online-Symptomchecker nationale Infektionstrends erfassen können und somit ein wertvolles Werkzeug für die Pandemiebekämpfung darstellen. Die Daten sind unter [github.com/symptoma/global_trends_symp_c19](https://github.com/symptoma/global_trends_symp_c19) verfügbar. [@zobel2023correlating]

Die Studie "Diagnostic Accuracy of Web-Based COVID-19 Symptom Checkers: Comparison Study" von Nicolas Munsch, Alistair Martin und weiteren Autoren, veröffentlicht im Oktober 2020 im Journal of Medical Internet Research (DOI: 10.2196/21299), bewertet die diagnostische Genauigkeit von zehn webbasierten COVID-19-Symptomcheckern. Sie analysiert deren Leistung anhand von 50 COVID-19-Fällen und 410 Kontrollfällen ohne COVID-19, wobei Sensitivität, Spezifität, F1-Score und Matthews-Korrelationskoeffizient (MCC) ermittelt wurden. Die Ergebnisse zeigen große Unterschiede: Symptoma (F1=0.92, MCC=0.85) und Infermedica (F1=0.80, MCC=0.61) erzielten die besten Werte für „hohes Risiko“, während andere wie Ada (F1=0.24) und Your.MD (F1=0.24) schlechter abschnitten. Die Studie hebt hervor, dass nur zwei Checker ein gutes Gleichgewicht zwischen Sensitivität und Spezifität bieten, und betont die Bedeutung solcher Tools für Triage und Entlastung des Gesundheitswesens während der Pandemie, trotz variabler Zuverlässigkeit. [@munsch2020diagnostic]

Die Studie "Comparison of Two Symptom Checkers (Ada and Symptoma) in the Emergency Department: Randomized, Crossover, Head-to-Head, Double-Blinded Study" von Johannes Knitza und Kollegen, veröffentlicht 2024 im Journal of Medical Internet Research (DOI: 10.2196/56514), vergleicht die diagnostische Genauigkeit, Sicherheit, Benutzbarkeit und Akzeptanz der Symptomchecker Ada und Symptoma in der Notaufnahme des Universitätsklinikums Erlangen. In einer randomisierten, doppelt verblindeten Crossover-Studie mit 437 Patienten zwischen April und November 2021 zeigte Ada eine höhere diagnostische Genauigkeit (identische Diagnose bei 14 % vs. 4 % für Symptoma als Top-Diagnose) und bessere Benutzbarkeit (88 % vs. 78 % fanden sie einfach). Beide Checker übersahen jedoch bei 13–14 % der Fälle potenziell lebensbedrohliche Diagnosen, und Ada triagierte 34 % korrekt, aber 13 % zu niedrig. Die Akzeptanz war gering (NPS: Ada –34, Symptoma –47). Die Autoren warnen vor der unkritischen Nutzung solcher Tools in Notfällen und fordern strengere klinische Evaluationsstudien. [@knitza2024comparison]

Die Studie "Vom Symptom zur Diagnose – Tauglichkeit von Symptom-Checkern: Update aus Sicht der HNO" von J. Nateqi und Kollegen, veröffentlicht am 16. April 2019 in HNO (Band 67, S. 334–342), untersucht die diagnostische Genauigkeit moderner Symptomchecker aus der Perspektive der Hals-Nasen-Ohren-Heilkunde. Sie aktualisiert eine Harvard-Studie von 2015, die eine Treffergenauigkeit von 29–71 % feststellte, indem sie fünf neue Checker (Symptoma, Ada, FindZebra, Mediktor, Babylon) einbezieht und die Ergebnisse normiert. Symptoma sticht mit 82,2 % (Top 1), 100 % (Top 3 und Top 10) heraus und übertrifft den bisherigen Standard deutlich. In einem HNO-spezifischen Test mit Fällen aus dem British Medical Journal erreicht Symptoma 64,3 % (Top 1), 92,9 % (Top 3) und 100 % (Top 10), weit vor Isabel (21,4 %; 40,5 %; 61,9 %) und FindZebra (26,2 %; 42,9 %; 54,8 %). Die Autoren schließen, dass Symptoma als einzige praxistaugliche Lösung gilt, empfehlen jedoch größere Studien, insbesondere zu seltenen Krankheiten. [@nateqi2019symptom]

Die Studie „Laypeople’s Use of and Attitudes Toward Large Language Models and Search Engines for Health Queries: Survey Study“ untersucht, wie Laien in den USA große Sprachmodelle (LLMs) wie ChatGPT und Suchmaschinen wie Google für Gesundheitsfragen nutzen, und zeigt Auswirkungen für die ambulante medizinische Versorgung. Während Suchmaschinen mit 95,6 % Nutzung die Hauptquelle bleiben, verwenden bereits 32,6 % LLMs, wobei 13,9 % diese sogar als erste Anlaufstelle wählen – ein Hinweis auf eine wachsende Akzeptanz, die den Zugang zu Gesundheitsinformationen erleichtert und die Patientenautonomie stärkt. [@info:doi/10.2196/64290]

Die Studie „The RepVig framework for designing use-case specific representative vignettes and evaluating triage accuracy of laypeople and symptom assessment applications“ von Marvin Kopka et al. (Scientific Reports, 2024) stellt den RepVig Framework vor, der repräsentative Vignetten für die Bewertung von Selbsttriage-Entscheidungen durch Laien, Symptom-Assessment-Apps (SAAs) und Large Language Models (LLMs) entwickelt. Basierend auf repräsentativen Designprinzipien wurden 45 Vignetten aus Reddit-Posts (Subreddit r/AskDocs) gesammelt und mit traditionellen, von Klinikern erstellten Vignetten verglichen. Die Ergebnisse zeigen, dass repräsentative Vignetten höhere Genauigkeit (OR=1.52-2.00), Sicherheit (OR=1.81-3.41) und Neigung zur Übertriage (OR=1.80-2.66) bei Laien, SAAs und LLMs erzielen, wobei sich die Rangfolge der besten SAAs und LLMs ändert. Die Autoren empfehlen, den RepVig Framework für zukünftige Studien zu nutzen, um realitätsnähere Vignetten zu erstellen und die Generalisierbarkeit von Triage-Leistungen zu verbessern. [@kopka2024repvig]

Die Studie „Technology-supported self-triage decision making“ von Marvin Kopka et al. (npj Health Systems, 2025) untersucht, wie Laien Symptom-Assessment-Apps (SAAs) und Large Language Models (LLMs) für Selbsttriage-Entscheidungen nutzen. Durch eine konvergente Mixed-Methods-Studie mit Interviews und einem randomisierten kontrollierten Versuch zeigt die Studie, dass Entscheidungsprozesse durch Faktoren vor, während und nach der Interaktion beeinflusst werden. Laien nutzen Technologie für Informationssammlung und -analyse, bleiben aber für die Integration und finale Entscheidung verantwortlich. Quantitative Ergebnisse zeigen, dass die Entscheidungsgenauigkeit mit einer leistungsstarken SAA (Ada) von 53,2 % auf 64,5 % steigt (OR=2,52, p<0,001), nicht jedoch mit ChatGPT (54,8 % vor vs. 54,2 % nach Nutzung, p=0,79). Die Autoren schlagen ein Modell für technologiegestützte Selbsttriage vor und betonen die Notwendigkeit, Mensch-Technologie-Teams statt isolierter Systeme zu untersuchen. [@kopka2025technology]

## Leistungsvergleich

Benchmarking ist ein systematischer Prozess, bei dem die Leistungsfähigkeit von Systemen, wie großen Sprachmodellen (LLMs), durch Vergleich mit einem definierten Standard gemessen wird, um Stärken und Schwächen zu identifizieren und Verbesserungen voranzutreiben. Der Artikel "DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models" veranschaulicht dies durch einen anspruchsvollen Benchmark, der die diagnostischen Fähigkeiten von LLMs in komplexen klinischen Szenarien testet. DiagnosisArena umfasst 1.113 strukturierte Fälle aus 28 medizinischen Fachbereichen, basierend auf Fallberichten aus Journalen wie Lancet und NEJM. Durch einen mehrstufigen Prozess aus Datensammlung, Segmentierung, iterativem Filtern und Experten-KI-Verifikation stellt der Benchmark sicher, dass nur komplexe Fälle mit ausreichenden diagnostischen Hinweisen enthalten sind. Modelle wie o3-mini (45,82 % Genauigkeit), o1 (31,09 %) und DeepSeek-R1 (17,79 %) zeigen erhebliche Schwächen, was eine Generalisierungslücke bei klinischen Diagnosen aufdeckt. Multiple-Choice-Formate vereinfachen die Aufgabe künstlich, da Modelle wie o1 hier 61,90 % erreichen, was ihre wahren Reasoning-Fähigkeiten nicht widerspiegelt. [@zhu2025diagnosisarenabenchmarkingdiagnosticreasoning]
